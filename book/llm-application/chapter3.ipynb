{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "86c20cc6-bc3e-41b8-870c-7f42b8c7bc4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in ./.venv/lib/python3.11/site-packages (2.4.1)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.11/site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./.venv/lib/python3.11/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in ./.venv/lib/python3.11/site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.11/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.11/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in ./.venv/lib/python3.11/site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.11/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0920f799-19d3-420b-ae74-d72c40ed9be3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a5682803-0335-4ca7-9ea4-f2f62aad762c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예제 3.1\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "text = \"What is Huggingface Transformers?\"\n",
    "# BERT 모델 활용\n",
    "bert_model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "encoded_input = bert_tokenizer(text, return_tensors='pt')\n",
    "bert_output = bert_model(**encoded_input)\n",
    "\n",
    "# GPT-2 모델 활용\n",
    "gpt_model = AutoModel.from_pretrained('gpt2')\n",
    "gpt_tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "encoded_input = gpt_tokenizer(text, return_tensors='pt')\n",
    "gpt_output = gpt_model(**encoded_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20781bd3-0a63-4228-95b6-c3417ced778e",
   "metadata": {},
   "source": [
    "## 예제 3.2 모델 아이디로 모델 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7f79b196-d792-4cda-9a0f-fc8194b4d7c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel\n",
    "model_id = 'klue/roberta-base'\n",
    "model = AutoModel.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86613c16-d390-4c70-82f4-2fa97f396271",
   "metadata": {},
   "source": [
    "#### 다운로드 받은 모델이 저장되는 경로"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "835d0cb4-597d-4ff8-a31f-4d5519aed59a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/avery/.cache/huggingface/hub\n"
     ]
    }
   ],
   "source": [
    "from transformers import file_utils\n",
    "\n",
    "print(file_utils.default_cache_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be980783-c282-4080-bd3a-0a351a8c9ed6",
   "metadata": {},
   "source": [
    "## 예제 3.4. 분류 헤드가 포함된 모델 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "19ebfd90-0f55-4e12-8e52-6bdaa4c0f89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "model_id = 'SamLowe/roberta-base-go_emotions'\n",
    "classification_model = AutoModelForSequenceClassification.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "57007221-9734-4c32-88bd-4540fca8e964",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'disapproval', 'score': 0.4287703335285187}, {'label': 'neutral', 'score': 0.3653065264225006}, {'label': 'disappointment', 'score': 0.13700710237026215}, {'label': 'annoyance', 'score': 0.12686915695667267}, {'label': 'sadness', 'score': 0.019367903470993042}, {'label': 'realization', 'score': 0.01827990636229515}, {'label': 'approval', 'score': 0.015486909076571465}, {'label': 'anger', 'score': 0.008378804661333561}, {'label': 'disgust', 'score': 0.003324487479403615}, {'label': 'embarrassment', 'score': 0.0026055388152599335}, {'label': 'confusion', 'score': 0.0023298130836337805}, {'label': 'optimism', 'score': 0.002093370072543621}, {'label': 'caring', 'score': 0.0020567646715790033}, {'label': 'joy', 'score': 0.001946124481037259}, {'label': 'desire', 'score': 0.0017248146468773484}, {'label': 'relief', 'score': 0.0013772249221801758}, {'label': 'surprise', 'score': 0.0013576180208474398}, {'label': 'nervousness', 'score': 0.00135464605409652}, {'label': 'admiration', 'score': 0.0012708209687843919}, {'label': 'remorse', 'score': 0.0011912663467228413}, {'label': 'amusement', 'score': 0.0011287438683211803}, {'label': 'excitement', 'score': 0.001094142091460526}, {'label': 'fear', 'score': 0.0009878521086648107}, {'label': 'gratitude', 'score': 0.0009266863344237208}, {'label': 'curiosity', 'score': 0.0007525130058638752}, {'label': 'grief', 'score': 0.0007474041194655001}, {'label': 'love', 'score': 0.0007149843731895089}, {'label': 'pride', 'score': 0.0004541154485195875}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(task=\"text-classification\", model=model_id, top_k=None)\n",
    "\n",
    "sentences = [\"I cannot focus on my study.\"]\n",
    "\n",
    "model_outputs = classifier(sentences)\n",
    "print(model_outputs[0])\n",
    "# produces a list of dicts for each of the labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80959050-3c1e-4682-8f56-f5a996e7579a",
   "metadata": {},
   "source": [
    "## 예제 3.6. 분류 헤드가 랜덤으로 초기화된 모델 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9e9d4ed2-12bd-4eb4-869b-fcbcd0559c20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "model_id = 'klue/roberta-base'\n",
    "classification_model = AutoModelForSequenceClassification.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62709de5-8328-4791-b7ff-5647be2e7acc",
   "metadata": {},
   "source": [
    "## 예제 3.8 토크나이저 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2fe4ad42-46a9-4f25-8b9e-429008ba772b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "model_id = 'klue/roberta-base'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269e3b6d-e0b6-425c-bedb-a533c3e56411",
   "metadata": {},
   "source": [
    "## 예제 3.9. 토크나이저 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "80824742-0f9b-496c-9c88-d67f1ccad8b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [0, 9157, 7461, 2190, 2259, 8509, 2138, 1793, 2855, 5385, 2200, 20950, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "['[CLS]', '토크', '##나이', '##저', '##는', '텍스트', '##를', '토', '##큰', '단위', '##로', '나눈다', '[SEP]']\n",
      "[CLS] 토크나이저는 텍스트를 토큰 단위로 나눈다 [SEP]\n",
      "토크나이저는 텍스트를 토큰 단위로 나눈다\n"
     ]
    }
   ],
   "source": [
    "tokenized = tokenizer(\"토크나이저는 텍스트를 토큰 단위로 나눈다\")\n",
    "print(tokenized)\n",
    "# {'input_ids': [0, 9157, 7461, 2190, 2259, 8509, 2138, 1793, 2855, 5385, 2200, 20950, 2],\n",
    "#  'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "#  'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
    "# https://huggingface.co/klue/roberta-base/raw/main/tokenizer.json 에서 input_ids 를 찾아면 어떤 단어인지 찾아볼 수 있다.\n",
    "\n",
    "print(tokenizer.convert_ids_to_tokens(tokenized['input_ids']))\n",
    "# ['[CLS]', '토크', '##나이', '##저', '##는', '텍스트', '##를', '토', '##큰', '단위', '##로', '나눈다', '[SEP]']\n",
    "\n",
    "print(tokenizer.decode(tokenized['input_ids']))\n",
    "# [CLS] 토크나이저는 텍스트를 토큰 단위로 나눈다 [SEP]\n",
    "\n",
    "print(tokenizer.decode(tokenized['input_ids'], skip_special_tokens=True))\n",
    "# 토크나이저는 텍스트를 토큰 단위로 나눈다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5769ac3-91d1-4cd6-a6d2-7c7fadacb771",
   "metadata": {},
   "source": [
    "## 예제 3.10 토크나이저 여러 문장 넣기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "dc23d7ae-51f9-467f-9142-23b15c1c06c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[0, 4988, 2031, 2073, 17034, 3864, 1521, 859, 2116, 2067, 2075, 2182, 35, 2], [0, 4442, 1535, 2259, 3944, 12305, 2097, 2118, 2088, 1513, 2203, 2182, 65, 65, 31, 31, 2]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer([\"여러분들은 스터디 준비 잘 되가시나요?\", \"요즘 저는 정말 헤이해지고 있네요^^;;\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a478b1b-ad6a-4e06-ac59-c518f082aa0d",
   "metadata": {},
   "source": [
    "## 예제 3.11. 하나의 데이터에 여러 문장이 들어가는 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e26b316c-bb18-4dcd-841c-cbbcf200d42b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[0, 4988, 2031, 2073, 17034, 3864, 1521, 859, 2116, 2067, 2075, 2182, 35, 2, 4442, 1535, 2259, 3944, 12305, 2097, 2118, 2088, 1513, 2203, 2182, 65, 65, 31, 31, 2]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer([[\"여러분들은 스터디 준비 잘 되가시나요?\", \"요즘 저는 정말 헤이해지고 있네요^^;;\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda93146-006b-4bcc-8145-6cb00f733740",
   "metadata": {},
   "source": [
    "## 예제 3.12. 토큰 아이디를 문자열로 복원"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5d904b2d-cfa8-46b5-bd18-c0c6b869e85e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS] 여러분들은 스터디 준비 잘 되가시나요? [SEP]',\n",
       " '[CLS] 요즘 저는 정말 헤이해지고 있네요 ^ ^ ; ; [SEP]',\n",
       " '[CLS] 순서대로 읽히나? [SEP]']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_tokenized_result = tokenizer([\"여러분들은 스터디 준비 잘 되가시나요?\", \"요즘 저는 정말 헤이해지고 있네요^^;;\", \"순서대로 읽히나?\"])['input_ids']\n",
    "tokenizer.batch_decode(first_tokenized_result)\n",
    "# [CLS] 여러분들은 스터디 준비 잘 되가시나요? [SEP]\n",
    "\n",
    "second_tokenized_result = tokenizer([\"여러분들은 스터디 준비 잘 되가시나요?\", \"요즘 저는 정말 헤이해지고 있네요^^;;\", \"순서대로 읽히나?\"])['input_ids']\n",
    "tokenizer.batch_decode(second_tokenized_result)\n",
    "# '[CLS] 요즘 저는 정말 헤이해지고 있네요 ^ ^ ; ; [SEP]'\n",
    "\n",
    "third_tokenized_result = tokenizer([\"여러분들은 스터디 준비 잘 되가시나요?\", \"요즘 저는 정말 헤이해지고 있네요^^;;\", \"순서대로 읽히나?\"])['input_ids']\n",
    "tokenizer.batch_decode(third_tokenized_result)\n",
    "# [CLS] 순서대로 읽히나? [SEP]\n",
    "\n",
    "# ['[CLS] 여러분들은 스터디 준비 잘 되가시나요? [SEP]',\n",
    "#  '[CLS] 요즘 저는 정말 헤이해지고 있네요 ^ ^ ; ; [SEP]',\n",
    "#  '[CLS] 순서대로 읽히나? [SEP]']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a3c1e9-e57a-42e5-8cb9-b7377091973a",
   "metadata": {},
   "source": [
    "## 예제 3.13. BERT 토크나이저와 RoBERTa 토크나이저"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0cc2c194-86c6-43ab-8c12-b895738626df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[0, 13755, 47, 70, 4568, 157, 13, 5, 892, 116, 2, 2, 574, 7223, 6, 38, 348, 57, 562, 269, 22414, 4, 48232, 48640, 2], [0, 10859, 134, 2, 2, 10859, 134, 2]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_tokenizer = AutoTokenizer.from_pretrained('klue/bert-base')\n",
    "bert_tokenizer([['여러분 스터디 준비 잘 되가시나요?', '요즘 저는 정말 헤이해지고 있네요^^;;'], \n",
    "                ['여기1', '여기2']])\n",
    "# {'input_ids': [[2, 1656, 1141, 3135, 6265, 3, 864, 1141, 3135, 6265, 3]],\n",
    "# 'token_type_ids': [[0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1]],\n",
    "# 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n",
    "\n",
    "roberta_tokenizer = AutoTokenizer.from_pretrained('klue/roberta-base')\n",
    "roberta_tokenizer([['여러분 스터디 준비 잘 되가시나요?', '요즘 저는 정말 헤이해지고 있네요^^;;'], \n",
    "                   ['여기1', '여기2']])\n",
    "# {'input_ids': [[0, 1656, 1141, 3135, 6265, 2, 864, 1141, 3135, 6265, 2]],\n",
    "# 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
    "# 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n",
    "\n",
    "en_roberta_tokenizer = AutoTokenizer.from_pretrained('roberta-base')\n",
    "en_roberta_tokenizer([\n",
    "    ['Are you all preparing well for the study?', \"Lately, I've been getting really lazy.^^;;\"],\n",
    "    ['here1', 'here1']\n",
    "])\n",
    "# {'input_ids': [[0, 9502, 3645, 2, 2, 10815, 3645, 2]],\n",
    "# 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1]]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7004c322-ae79-454d-96ca-27e53e683119",
   "metadata": {},
   "source": [
    "## 예제 3.14. attention_mask 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fbfe33f1-00ab-4c8a-9435-bbfe3847e922",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[0, 1656, 1141, 3135, 6265, 2073, 1599, 2062, 18, 2, 1, 1, 1, 1, 1, 1], [0, 864, 1141, 3135, 6265, 2073, 1656, 1141, 3135, 6265, 3632, 831, 647, 2062, 18, 2]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(['첫 번째 문장은 짧다.', '두 번째 문장은 첫 번째 문장 보다 더 길다.'], padding='longest')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bd51a3-4c33-44a2-b762-7fc506f25ccc",
   "metadata": {},
   "source": [
    "## 예제 3.15. KLUE MRC 데이터셋 다운로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3e31061e-7aa8-44ff-bbfd-9645b23be47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "klue_mrc_dataset = load_dataset('klue', 'mrc')\n",
    "# klue_mrc_dataset_only_train = load_dataset('klue', 'mrc', split='train')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed7d5cf-27c4-4071-a84c-25d2823bf847",
   "metadata": {},
   "source": [
    "## 예제 3.16. 로컬의 데이터 활용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "46db46ff-1b65-47f0-ad40-657260080984",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "# 로컬의 데이터 파일을 활용\n",
    "dataset = load_dataset(\"csv\", data_files=\"my_file.csv\")\n",
    "\n",
    "# 파이썬 딕셔너리 활용\n",
    "from datasets import Dataset\n",
    "my_dict = {\"a\": [1, 2, 3]}\n",
    "dataset = Dataset.from_dict(my_dict)\n",
    "\n",
    "# 판다스 데이터프레임 활용\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "df = pd.DataFrame({\"a\": [1, 2, 3]})\n",
    "dataset = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a527b71-9ab8-4cdd-925a-576da65cc649",
   "metadata": {},
   "source": [
    "# 3.4 모델 학습시키기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a0c2db-b8d8-49cd-bdc4-c9213f4fd539",
   "metadata": {},
   "source": [
    "## 예제 3.17. 모델 학습에 사용할 연합뉴스 데이터셋 다운로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "87c0f166-c247-454d-8d75-a6c61421db43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['guid', 'title', 'label', 'url', 'date'],\n",
       "    num_rows: 45678\n",
       "})"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "klue_tc_train = load_dataset('klue', 'ynat', split='train')\n",
    "klue_tc_eval = load_dataset('klue', 'ynat', split='validation')\n",
    "klue_tc_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "fd35f429-3b47-4be3-aab5-35f92418897c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'guid': 'ynat-v1_train_00000',\n",
       " 'title': '유튜브 내달 2일까지 크리에이터 지원 공간 운영',\n",
       " 'label': 3,\n",
       " 'url': 'https://news.naver.com/main/read.nhn?mode=LS2D&mid=shm&sid1=105&sid2=227&oid=001&aid=0008508947',\n",
       " 'date': '2016.06.30. 오전 10:36'}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "klue_tc_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8c7a5f25-d311-44ef-aac2-82e55dd7169a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['IT과학', '경제', '사회', '생활문화', '세계', '스포츠', '정치']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "klue_tc_train.features['label'].names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36fbb7d-e0a3-4e2f-a265-4494c57d0fdb",
   "metadata": {},
   "source": [
    "## 예제 3.18. 실습에 사용하지 않는 불필요한 컬럼 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b3900215-2f6f-4022-9638-978e1648448f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['title', 'label'],\n",
       "    num_rows: 45678\n",
       "})"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "klue_tc_train = klue_tc_train.remove_columns(['guid', 'url', 'date'])\n",
    "klue_tc_eval = klue_tc_eval.remove_columns(['guid', 'url', 'date'])\n",
    "klue_tc_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ce6a99-92cc-40f9-a2af-a42476150862",
   "metadata": {},
   "source": [
    "## 예제 3.19. 카테고리를 문자로 표기한 label_str 컬럼 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6444b6f5-7b03-4de4-8c46-e9577c2d6041",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': '유튜브 내달 2일까지 크리에이터 지원 공간 운영', 'label': 3, 'label_str': '생활문화'}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "klue_tc_train.features['label']\n",
    "# ClassLabel(names=['IT과학', '경제', '사회', '생활문화', '세계', '스포츠', '정치'], id=None)\n",
    "\n",
    "klue_tc_train.features['label'].int2str(1)\n",
    "# '경제'\n",
    "\n",
    "klue_tc_label = klue_tc_train.features['label']\n",
    "\n",
    "def make_str_label(batch):\n",
    "  batch['label_str'] = klue_tc_label.int2str(batch['label'])\n",
    "  return batch\n",
    "\n",
    "klue_tc_train = klue_tc_train.map(make_str_label, batched=True, batch_size=1000)\n",
    "\n",
    "klue_tc_train[0]\n",
    "# {'title': '유튜브 내달 2일까지 크리에이터 지원 공간 운영', 'label': 3, 'label_str': '생활문화'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e6cebd-461e-482a-8f2e-d75e062e0e0c",
   "metadata": {},
   "source": [
    "## 예제 3.20. 학습/검증/테스트 데이터셋 분할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "82181b61-a14b-41f9-89f8-170a1c3e2d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = klue_tc_train.train_test_split(test_size=10000, shuffle=True, seed=42)['test']\n",
    "dataset = klue_tc_eval.train_test_split(test_size=1000, shuffle=True, seed=42)\n",
    "test_dataset = dataset['test']\n",
    "valid_dataset = dataset['train'].train_test_split(test_size=1000, shuffle=True, seed=42)['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4188c77-6cca-489f-93da-0037713bf17a",
   "metadata": {},
   "source": [
    "## 예제 3.21. Trainer를 사용한 학습: (1) 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9cd2348d-bb37-464e-ae50-6d46bb6c6579",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import (\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer\n",
    ")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"title\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "model_id = \"klue/roberta-base\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_id, num_labels=len(train_dataset.features['label'].names))\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "valid_dataset = valid_dataset.map(tokenize_function, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42d4103-aa04-4ca2-9434-c356b517215a",
   "metadata": {},
   "source": [
    "## 예제 3.22. Trainer를 사용한 학습: (2) 학습 인자와 평가 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "371b1004-a365-459f-8d40-50569d518172",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/avery/workspace/til/book/llm-application/.venv/lib/python3.11/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    push_to_hub=False\n",
    ")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return {\"accuracy\": (predictions == labels).mean()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43fcaf2-83bd-43fc-a4b1-a9f84bf2690e",
   "metadata": {},
   "source": [
    "## 예제 3.23. Trainer를 사용한 학습 - (3) 학습 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ca7bc3ae-6b76-480a-a772-c0b8e117eb7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1250' max='1250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1250/1250 58:33, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.535200</td>\n",
       "      <td>0.559551</td>\n",
       "      <td>0.841000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [125/125 01:44]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.5108214020729065,\n",
       " 'eval_accuracy': 0.843,\n",
       " 'eval_runtime': 105.4904,\n",
       " 'eval_samples_per_second': 9.48,\n",
       " 'eval_steps_per_second': 1.185,\n",
       " 'epoch': 1.0}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=valid_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "trainer.evaluate(test_dataset) # 정확도 0.84"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea444f2-f9fb-4b1f-a94c-8b18f21aa3ab",
   "metadata": {},
   "source": [
    "## 예제 3.24. Trainer를 사용하지 않는 학습: (1) 학습을 위한 모델과 토크나이저 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e2630ec0-ece6-4abe-bbfa-87af359c18e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(32000, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=7, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW\n",
    "\n",
    "def tokenize_function(examples): # 제목(title) 컬럼에 대한 토큰화\n",
    "    return tokenizer(examples[\"title\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "# 모델과 토크나이저 불러오기\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_id = \"klue/roberta-base\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_id, num_labels=len(train_dataset.features['label'].names))\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91cd407-efd8-4b15-b24a-74337942e4f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5f88b5-2510-4eef-aecb-6b17a68c346a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6afd3e-4de5-41e4-b72c-cc81cebafe5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbc7289-ff7e-45a9-9cbd-257dc6f7b647",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa79c73-d7a3-4ddd-9b6c-96950ae077d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e670ecaa-4f1e-4692-8887-592807bd71e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584f70b1-43e8-40d5-9dae-15c976579106",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a13664-96b2-4d92-93b5-f05a2a37e114",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acf2b5f-8fdd-4574-820c-4bb3775c5e05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b17f35-92fc-45ab-bca8-ac27eb12a2f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3abeb62-3f77-4ef9-bbb4-da09ef03dfdf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe10d35-2db7-4e46-a622-c3f079aac613",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036227e2-9882-472b-9a13-a724c93d3ce8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
